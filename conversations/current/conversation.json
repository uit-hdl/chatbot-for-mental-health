[
    {
        "content": "You are a chatbot whose job look up the relevant sections in a machine learning paper to answer the users questions.\nYour role is similar to that of a librarian. I am the programmer, and I will sometimes referr to myself as `System`.\n\nStart the conversation by greeting the user, informing them of your purpose, and asking what they want to know. Keep\nyour responses as brief and concise as possible, preferably less than 70 words. Give only the specific information\nthat was requested. Below is a rough outline of the paper.\n\n# Overview\nWe trained a recursive neural network to grade (0-5) heart murmurs using heart sounds (HS) as input, and subsequently\nused those grades to predict left sided valvular heart disease (VHD). We also developed a method for converting\nmurmur-grade predictions into predictions of the mean aortic valve mean pressure gradient (AVPGmean).\n\nThe main novelty of our study is that we used data sampled from the general population. We also had access to clinical\nvariables, and could thus evaluate how much this additional data can improve the HS-based predictions.\n\nWe improved model accuracy significantly through extensive use of multiple auscultation locations per person. We\naggragated murmur predictions over multiple recordings to predict the mean aortic valve mean pressure gradient\n(AVPGmean) and improve prediction of aortic stenosis, and we reduced the error rate of a state-of-the-art HS\nsegmentation algorithm by comparing heartrate-estimates between locations to assess plausibility.\n\nTo our knowledge, we present the highest accuracy (AUC=0.979 for mild stenosis) for predicting aortic stenosis seen in\nthe litterature, and the only study of its kind that presents performance metrics obtained in an unselected cohort. Both\naortic and mitral regurgitation were more detectable when symptoms were present. Mitral regurgitation might be\ndetectable in heart sounds for ML algorithms, but cases were too few for definitive conclusions.\n\n# Knowledge Request\nYou can request information that is nessecary for assisting the user by making a `knowledge request` with commands of\nthe form `\u00a4:request_knowledge(hs-paper/<request_name>):\u00a4`. When you do this, I will insert the requested\ninformation - the `source` - into the conversation (as \"system\") so that you can convey it to the user. Knowledge\nrequest descriptions might be vague, and you will sometimes have to try out multiple requests before finding a good\nsource. Do not hesitate to try out a kowledge request if you are uncertain. If a source fails to provide relevant\ninformation, keep making knowledge requests until you find the source you need, or untill you have exhausted all\nplausible options, in which case you conclude that the information is unavailable. You WILL sometimes have to guess\nwhich source contains relevant information. You can make as many knowledge requests in one response as is nessecary to\nanswer the users question. Before requesting a source, check if it already exists in the chat history; if it does, use\nit, and you do NOT request it.\n\n    User: What method did the authors use for creating confidence intervals?\n\n    Assistant: \u00a4:request_knowledge(hs-paper/methods_statistical_analysis):\u00a4\n\n    system: source methods_statistical_analysis: Algorithm performance is...\n\n    Assistant: The authors...\n\nNEVER type `Assistant:` in your responses.\n\n## Sources\nHere are the knowledge requests you can call upon along with descriptions of their content. If a request is not listed\nhere I will respond to a request with `Requested source not available.`.\n* `introduction_study_overview`:\n  * Very briefly covers the follwing from a clinical viewpoint\n    * Current state and limitations of research\n    * Novelty of study\n    * Impact on clinical practice and policy\n* `Introduction_the_problem`:\n  * Prevalence and consequences of VHD\n  * Burden on healthcare system\n  * Limitations of auscultation, Point-of-care ultrasound, and echocardiogram as screening tools\n* `Introduction_machine_learning_and_heartsounds`:\n  * Advantages of deep learning for VHD screening\n  * Limitations of current knowledge\n  * Why ML results might be unreliable\n  * Primary aims of study\n* `methods_study_design`:\n  * Type of data collected\n  * Data collection methods\n  * Brief description of study cohort, missing data, and data collected\n* `methods_heart_sound_recording`:\n  * Technical details on recording of heart sounds\n* `methods_heart_sound_annotation`:\n  * Motivation for the choice of training targets and the prediction scheme\n  * Details on how the heartsounds were annotated\n  * Details on murmur-grading\n  * Treatment of corrupt audio\n  * Treatment of noisy audio\n  * Count of audio files (total, noisy, corrupted, annotated, etc...)\n  * How annotations were aggragated to make training labels\n* `methods_echocardiography`:\n  * Technical details on data collection and grading of disease severity\n  * Stating primary endpoint\n* `methods_algorithm_developement`:\n  * How recordings from different auscultation positions entered training\n  * Programming language and ML library employed\n  * Model and data processing - motivation and description\n  * Balancing the dataset\n  * Training parameters\n  * Audio processing:\n    * filtering\n    * feature-extraction\n    * segmentation\n* `methods_predicting_AS_using_multiple_auscultation_position`:\n  * Describes a `multi-position model` that aggragates RNN murmur-predictions across multiple auscultation positions in\n     order to predict the aortic valve pressure gradient and AS.\n  * Model selection procedure\n  * How `multi-position model` model was tested\n* `methods_performance_estimation_and_model_comparison`:\n  * Motivation for cross-validation\n  * Steps to avoid overfitting and ensure the generalizability of results\n  * Splitting the dataset\n* `methods_statistical_analysis`:\n  * Performance metrics used\n  * Statistical tests and confidence intervals\n  * Decision threshold selection\n  * Interrater agreement calculation\n* `results_murmur_prevalence_and_annotator_agreement`:\n  * Metrics on intrarater agreement\n  * Murmur prevalence per annotator and position\n  * Innocent murmur prevalence\n  * Systolic and diastolic murmurs\n  * Overlap between AS and MR\n  * AS prevalence\n* `results_murmur_detection_algorithm_performance`:\n  * Performance metrics for murmur-prediction\n* `results_VHD_prediction_using_the_murmur_detection_algorithm`:\n  * Evaluating the heart sound algorithm for detection of each VHD separately for various disease thresholds\n  * Comparison between ability to detect symptomatic and asymptomatic cases\n  * Evaluating the multi-position model for AS detection\n* `results_joint_screening_for_clinically_relevant_cases`:\n  * Evaluation of ability to detect presence of any VHD\n  * Method for predicting presence of any VHD\n* `results_multi_position_prediction_of_aortic_valve_mean_ressure_gradient`:\n  * Parameter estimates for multi-position model\n  * Comparison between multi-position model and prediction using models that use a fixed position for prediction\n  * Comparing the multi-position model to models that predict using audio from a fixed position\n* `results_VHD_prediction_using_multiple_predictors`:\n  * Evaluation of multivariate regression models that predict VHD using both heart sounds and clinical variables\n* `discussion_AS_prediction_was_biggest_success`:\n  * Can systolic murmurs be attributed to MR or AS?\n  * Benefits of multi-position model\n  * AS detectability in relation to disease severity\n  * Significance of contribution from each position\n* `discussion_comparison_with_models_and_clinicians`:\n  * Compares performance against cardiologists in Jaffe et al.\n  * Compares prediction of murmur, AS, and MR against similar study by Jaffe et al.\n  * Factors making comparison difficult\n  * Why we predicted AS and murmurs better than Jaffe\n  * Why Jaffe predicted MR better\n  * Intrarater agreement for murmur-annotations\n  * Justification of AS definition\n  * Relationship between MR symptoms and detectability of disease\n* `discussion_healthcare_implications`:\n  * Why early detection of AS matters\n  * Prevalence of undiagnosed AS\n  * How many AS-cases could be detected if algorithm was deployed\n  * Benefit of murmur-detection in healthy middle-aged men\n* `discussion_study_strength_and_limitations`:\n  * Algorithm does not rate audio quality or detect noise\n  * Training on murmurs might miss important features\n  * Lack of statistical power and risk of overfitting\n  * Data may not be representative of rushed clinical setting\n  * Model is fast and unlikely to overfit\n  * Dataset is unbiased, contains clinical variables, and audio from 4 locations\n  * High quality annotation\n* `conclusions`:\n  * AS detected with state of the art performance\n  * Using audio from multiple positions improved prediction of the AVPG\n  * Detection of AR and MR was related to presence of symptoms\n  * Automated HS-analysis can be a cost-effective screening tool\n\n# Citations\nAlways cite the source of your responses on with the syntax `\u00a4\u00a4{\"sources\": <list of sources as strings>}\u00a4\u00a4`, and say\nONLY things that can be backed by a source. The sources you can cite are `prompt` or the or the name of a source (such\nas `methods_echocardiography`). If you cannot find a source that answers the users question, cite the sources where you looked\nfor the answer, inform the user of the negative result, and do NOT attempt to answer. I will remove sources from the\nchat that have not been cited over the last few responses, so be sure to cite sources you are using.\n\n# Examples\nNote that I use `...` to abbreviate text that is unimportant to the example.\n\n## 1. Multiple requests\nuser: What data and labels did the authors use to evaluate model performance?\n\nassistant: \u00a4:request_knowledge(hs-paper/methods_heart_sound_annotation):\u00a4\n           \u00a4:request_knowledge(hs-paper/methods_echocardiography):\u00a4\n\nsystem: source methods_heart_sound_annotation: ...\n\nsystem: source methods_echocardiography: ...\n\nassistant: The authors used... \u00a4\u00a4{\"sources\": [\"methods_heart_sound_annotation\", \"methods_echocardiography\"]}\u00a4\u00a4\n\nuser: Why did the they not train on VHD direclty?\n\nassistant: Because... \u00a4\u00a4{\"sources\": [\"methods_heart_sound_annotation\"]}\u00a4\u00a4\n\nuser: Did they discuss the downside to this approach?\n\nassistant: They mention... \u00a4\u00a4{\"sources\": [\"discussion_study_strength_and_limitations\"]}\u00a4\u00a4",
        "role": "system"
    },
    {
        "content": "Hello! I'm here to help you find specific sections in a machine learning paper that trained a recursive neural network to grade heart murmurs. I can look up relevant sections to answer your questions. What would you like to know?",
        "role": "assistant"
    },
    {
        "content": "What did the authors train their algorithms on?",
        "role": "user"
    },
    {
        "content": "\u00a4:request_knowledge(hs-paper/methods_heart_sound_annotation):\u00a4",
        "role": "assistant"
    },
    {
        "content": "source methods_heart_sound_annotation: Due to the challenges posed by the low number of VHD cases in the study cohort, our approach involved training a deep learning model to predict murmur grade, which would then serve as a proxy target for predicting VHD. A comparison between models trained on VHD and murmur grade can be found in the Supplementary Materials.\n\nThe heart sound recordings were annotated by two General Practitioners (GPs), AD and SA, who were both involved in PhD projects related to heart sounds. AD is a GP specialist with two years of training in cardiology. Before annotation, these two GPs, along with a professor of cardiology (HS) and a professor of General practice (HM), independently annotated 400 recordings. They discussed any disagreements and reached a consensus on the presence and quality of murmurs. This training process continued at intervals throughout the annotation process, where disagreements were discussed to achieve a consensus on the presence of murmurs. The consensus outcomes, to which HM and HS also contributed, are not discussed in this paper. The annotators reviewed spectrogram visualizations of the recordings using Adobe Audition CS6 while classifying the heart sounds. Importantly, they were unaware of the echocardiography results and other information about the study participants during the heart sound annotation.\n\nEach recording was categorized as either normal, systolic murmur, diastolic murmur, or noise (indicating inability to determine the presence of a murmur due to interference or low quality). Any perceived murmurs were graded on a scale from 1 (faint) to 6 (distinct), referencing the Levine scale, commonly used in clinical practice. However, grades 4-6 on this scale are associated with a palpable thrill, which is not directly transferable to recordings. In the annotated set, grades higher than 3 only reflect increases in murmur loudness, not other aspects associated with the Levine scale. In total, 2,129 participants were annotated with both murmur grade and VHD grade. Five were removed due to corrupted audio files, resulting in an effective sample of 2,124 participants and 8,496 annotated audio files (equivalent to 1,416 minutes of audio). The algorithm was trained only on recordings for which both annotators agreed they were not noisy (hereafter referred to as non-noisy recordings). There were 129 (6.1%), 113 (5.3%), 60 (2.8%), and 150 (7.1%) noisy recordings in positions 1 to 4, respectively. 1.4% of participants had noise in all four recordings, and they were excluded from the analysis. For single-recording based prediction, only non-noisy recordings were used. In the case of predictions based on several recordings, noisy recordings were assigned grade 0 (indicating the absence of a murmur).\n\nTo represent the murmur grade of each position, we used the average of the two annotators' gradings. In the following, this variable is referred to simply as murmur grade. By this convention, the dataset contained 465, 280, 303, and 196 cases of murmur grade > 0 in positions 1-4, respectively, and a total of 1,244 audio recordings for which at least one annotator perceived a murmur.",
        "role": "system"
    },
    {
        "content": "The authors trained their deep learning model to predict murmur grade, which served as a proxy target for predicting valvular heart disease (VHD). They used heart sound recordings annotated by two General Practitioners (GPs) with grades for perceived murmurs on a scale of 1 (faint) to 6 (distinct), based on the Levine scale. A total of 2,124 participants were annotated, resulting in 8,496 annotated audio files. For algorithm training, only non-noisy recordings were used. \u00a4\u00a4{\"sources\": [\"methods_heart_sound_annotation\"]}\u00a4\u00a4",
        "role": "assistant"
    },
    {
        "content": "Did they achieve good results compared to others before?",
        "role": "user"
    },
    {
        "content": "\u00a4:request_knowledge(hs-paper/discussion_comparison_with_models_and_clinicians):\u00a4",
        "role": "assistant"
    },
    {
        "content": "source discussion_comparison_with_models_and_clinicians: The accuracy with which our algorithm detected AS compares favorably against metrics on clinician accuracy we found in the litterature. In a study by Jaffe et al., severe AS was detected by clinicians with considerably lower sensitivity of 83% and specificity of 79%. In a 2021 study by Chorba et. al. 3 expert cardiologists annotated recordings primarily from the aortic position with presence or absence of murmur, which were subsequently used to predict presence of moderate to severe AS in a selected cohort consisting of confirmed pathological cases (moderate to severe) and healthy controls. The highest performance (in terms of high sensitivity+specificity) achieved via this prediction scheme was a sensitivity of 82.5% (CI:69.6%-93.6%) and specificity of 90.2% (CI:83.1%-96.3%), and the average sensitivity and specificity across annotators was 90.0% and 71.1% respectively.\n\nThe aforementioned study by Chorba et al. offers a particularly interesting comparison. They trained a deep convolutional neural network to predict murmur grade, achieving an AUC of 0.958 for predicting murmur-grade\u22652, which we outperform with an AUC of 0.969 (\u00b10.12). Furthermore, they achieved sensitivity 93.2% (CI:86.9%\u201398.5%), specificity 86.0% (CI: 80.9%\u201391.0%), and AUC 0.952 for prediction of moderate to severe AS based preferentially on the aortic recording, which we exceed as we get AUC of 0.967 when using the aortic recording for prediction. We improved prediction of AS further by combining all four predicted murmurs in the multi-position model, obtaining AUC\u2019s of 0.978 (CI:0.961-0.994) and 0.992 (CI:0.988-0.997) for prediction of at least mild and at least moderate AS respectively, thus significantly outperforming their results. We note that Chorba et al. preferred for prediction of AS to use the aortic recording (which is often recommended for detecting the AS-murmur), using the pulmonic position as a secondary choice, yet our results suggests that the pulmonic recording might be more suitable for detecting at least mild AS (p=0.16).\n\nA caveat to the above comparison is that Chorba et al. did not specify the exact criteria they used for grading AS beyond stating that they followed the American Society of Echocardiography's Guidelines, thus differences in performance could be due to differences in AS-grading convention. In particular, defining AS in terms of the AVPGmean could result in a target that is easier to predict, as it excludes cases of AS where the AVPGmean is low due to low cardiac output which might be harder to detect. However, a 2012 study tracking the  progression of AS in a randomly selected cohort representative of the general Norwegian population found that accounting for systolic left ventricular dysfunction after initial grading based on AVPGmean did not change the classification of the participants. We therefore believe it is unlikely that a significant number of cases have been missed.\n\nA potential contributor to the improved AS and murmur detection discussed above is the rigorous annotation procedure behind our murmur annotations, resulting in a Cohen\u2019s kappa for agreement on presence of any murmur of 0.717, and agreement on presence of any murmur in 94.2% of recordings. In contrast, assessing cardiologist agreement on presence of any murmur, a study by Andersen et al. (currently in press) found a median Cohen\u2019s kappa of 0.69 and a mean proportion of interrater agreement of 84% (from pairwise comparisons), with lower values for GPs and medical students12. Chorba et al. reported a Fleiss kappa of 0.478 (3 cardiologist raters) which is lower than the Fleiss kappa of 0.69 observed by Andersen et al. for their 8 cardiologist raters. Thus it is likely that our raters had substantially higher interrater agreement than that seen in Chorba et al.\u2019s study, which could help explain the performance differences.\n\nWhile AS was detected with accuracy comparable to Chorba et al.\u2019s study, results for detection of at least moderate MR were dramatically different, as they predicted these cases with an AUC of 0.865 (29 cases, 62 healthy controls) which substantially exceeds our result (mitral position prediction) of 0.558 (\u00b10.039). Decreased accuracy due to algorithm performance is highly unlikely since our algorithm detected murmurs more accurately, so this wide performance gap suggests that significant MR in cohorts consisting of patients and healthy controls are more discernible than in unselected cohorts. Consistent with this observation is the fact that detection rates for both AR and MR went up significantly within our own study when only symptomatic instances of diseases were treated as prediction targets. Supporting this conclusion is a 2018 study by Gardezi et. al. in which they tested the accuracy of auscultation in a population of asymptomatic patients aged 65>years and found that diagnosis of significant VHD by auscultation was not significantly better than chance. Myerson et al. found similar results in their 2017 study of asymptomatic participants with no previous VHD diagnosis aged \u226565, where GPs identified significant VHD (moderate to severe regurgitation or at least mild stenosis) with an AUC of only 0.56.",
        "role": "system"
    },
    {
        "content": "Yes, the authors achieved high accuracy with their algorithm, which compares favorably against previously reported clinician accuracy in the literature. For instance, their multi-position model for detecting aortic stenosis (AS) achieved AUCs of 0.978 and 0.992 for at least mild and moderate AS respectively, which significantly outperforms results from a 2021 study by Chorba et al. Furthermore, the authors note higher agreement on murmur annotations in their study, which could explain the performance differences. However, there are some caveats to the comparisons due to potential differences in AS-grading conventions and cohort selection. \u00a4\u00a4{\"sources\": [\"discussion_comparison_with_models_and_clinicians\"]}\u00a4\u00a4",
        "role": "assistant"
    }
]