word_wrap: 80  # N.o. characters before line gets wrapped.
plot_duration: 10000  # The duration of time for which the figures are shown (uggly solution...)
max_tokens_before_truncation: 4500  # Determines maximum length before chat gets truncated
inactivity_threshold: 2  # Remove source from conversation if it has not been cited in the last ... messages
n_attempts_at_producing_valid_response: 1
max_requests: 2
# Printing
print_knowledge_requests: True

# --- Pricing info for each model ---
dollars_per_1k_token:
  "gpt-4":
    "input": 0.03
    "output": 0.06
  "gpt-3.5-turbo":
    "input": 0.0005
    "output": 0.0015
  "gpt-3.5-turbo-1106":
    "input": 0.0005
    "output": 0.0015
  "gpt-3.5-turbo-instruct":
    "input": 0.0015
    "output": 0.002
dollars_per_1k_output_token: 0.06
nok_per_dollar: 10.6  # rate: NOK per Dollar

# --- TOKEN LIMITS AND BOUNDARIES ---
max_tokens_per_message: 270  # Limit in prompt + margin
max_tokens_hard_upper_limit: 320  # This is an argument in ChatCompletion.create() used to
max_tokens_single_response_bots: 90  # Max tokens for GPT-3.5-turbo instruct
# estimate token consumption rate and truncate messages. Set abit higher to avoid truncation due to
# differences in how N-tokens is counted
limit_1_tokens_per_message: 150  # Mild warning
limit_2_tokens_per_message: 200  # More severe warning
max_tokens_before_summarization: 201  # Bot message gets shortened by GPT-3.5
warning_limit_tokens_per_message: 150

# --- OVERSEER BOTS ---
# Set to false to turn of filters that check the contents of generated messages
enable_overseer_filter: False
enable_hard_coded_filter: False